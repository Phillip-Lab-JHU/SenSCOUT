{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b16a106",
   "metadata": {},
   "source": [
    "# Xception Model for Senescence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb4f4b",
   "metadata": {},
   "source": [
    "This jupyter notebook will take into account training of the Xception model and architecture. Training structure (i.e. splits on preprocessing,training functions)  was derived from DeepSesmo (https://doi.org/10.1038/s41467-020-20213-0). Xception was used as the base architecture with appended global averaging, dropout, and softmax dense layer. Note this platform is trained on 3-dimensional images (256,256,3). For this work, we used a nuclei channel, actin channel, and composite channel respectively. This can be modulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3948d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as crv\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix,  roc_curve, auc\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import gradient_descent_v2\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.inception_v3 import InceptionV3 \n",
    "from tensorflow.keras.applications.resnet50 import ResNet50 \n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model, model_from_json, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D, GlobalMaxPooling2D, Dropout, Flatten,Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.initializers import glorot_normal, glorot_uniform, he_normal, he_uniform, lecun_normal, lecun_uniform\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7888654",
   "metadata": {},
   "source": [
    "Preprocessing function below. Since our images are 8 bit (maximum pixel value of 255), this function will standardize alll pixels between -1 and 1 for the machine learning inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3517a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x0):\n",
    "    return ((x0/255.)-0.5)*2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e673e",
   "metadata": {},
   "source": [
    "The following fdunction takes the two conditions (i.e. senescent and non senescent) and creates train-test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddba1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setting(condition_1, condition_2):\n",
    "    \n",
    "    global test_size\n",
    "    \n",
    "    X_all = np.concatenate((condition_1, condition_2),axis=0)\n",
    "    y_all = np.concatenate((np.tile(np.array([[0]]),(condition_1.shape[0],1)),\n",
    "                             np.tile(np.array([[1]]),(condition_2.shape[0],1))\n",
    "                            ),axis=0)\n",
    "    train_X, test_X, train_y, test_y = crv.train_test_split(X_all,y_all,test_size=test_size,random_state=42)\n",
    "    \n",
    "    X_train = preprocess_input(np.copy(train_X))\n",
    "    X_test = preprocess_input(np.copy(test_X))\n",
    "    y_train = np.copy(train_y)\n",
    "    y_test = np.copy(test_y)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], scale_v*2, scale_h*2, n_chan)\n",
    "    Y_train = np_utils.to_categorical(y_train, 2)\n",
    "    X_test = X_test.reshape(X_test.shape[0], scale_v*2, scale_h*2, n_chan)\n",
    "    Y_test = np_utils.to_categorical(y_test, 2)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e212dd8",
   "metadata": {},
   "source": [
    "The following is the model architecture with xception with imagenet initialized weights. Addition of Global pooling, dropout, and Dense softmax layer at the end. Categorical crossentropy loss with adam optimizer (e-3) and accuracy metrics used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4861a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN_2():\n",
    "   \n",
    "    global nb_epoch, opt, my_init,base_model\n",
    "    \n",
    "    \n",
    "    base_model = Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(256, 256, 3),\n",
    "    include_top=False)  \n",
    "    \n",
    "    base_model.trainable = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs = keras.Input(shape=(256, 256, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    # Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x) \n",
    "    outputs = keras.layers.Dense(2, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    \n",
    "\n",
    "     \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0886e",
   "metadata": {},
   "source": [
    "Following plots model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8d1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    plt.plot(history.history['accuracy'],\"o-\",label=\"accuracy\")\n",
    "    plt.plot(history.history['val_accuracy'],\"o-\",label=\"val_acc\")\n",
    "    plt.title('model accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'],\"o-\",label=\"loss\",)\n",
    "    plt.plot(history.history['val_loss'],\"o-\",label=\"val_loss\")\n",
    "    plt.title('model loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942081c9",
   "metadata": {},
   "source": [
    "The following is a nested function in the 'Training CNN' master function. The datagen function keeps non-transfomative orientation shifts to the dataset (not we did not want to create shape-based transformations on the dataset). The function then trains the model on the resulting dataset for the originally outlined epochs. After this initial training, the base model is unfrozen and trained for additional epochs at a lower adams learning rate (e-4) to allow for finer convergance without overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34833c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_CNN(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    global batch_size, nb_epoch, model, history_x, shift_range, std_normalization, whitening,history_y\n",
    "    \n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=360,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        )\n",
    "\n",
    "    history_x = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
    "    steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "    epochs=nb_epoch,\n",
    "    validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),\n",
    "    validation_steps=X_test.shape[0]//batch_size)\n",
    "    \n",
    "    \n",
    "    #unfreeze weights for small amount of change \n",
    "    base_model.trainable = True\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=keras.optimizers.adam_v2.Adam(learning_rate=1e-4),\n",
    "             metrics=['accuracy'])\n",
    "    history_y=model.fit(datagen.flow(X_train, Y_train, batch_size=batch_size, shuffle=True),\n",
    "    steps_per_epoch=X_train.shape[0]//batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=datagen.flow(X_test, Y_test, batch_size=batch_size),\n",
    "    validation_steps=X_test.shape[0]//batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257b88b",
   "metadata": {},
   "source": [
    "The following is the master function that leverages the previous functions to train the xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6493541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_CNN(condition_1, condition_2):\n",
    "    \n",
    "    global nb_classes, batch_size, model, history_x, Y_test, y_pred, X_test,history_y\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = data_setting(condition_1, condition_2)\n",
    "    \n",
    "    model = model_CNN_2()\n",
    "    \n",
    "    do_CNN(X_train, X_test, Y_train, Y_test)\n",
    "    Y_pred = model.predict(X_test, batch_size=1,verbose=1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    print('final accuracy:',f1_score(np.argmax(Y_test,1), y_pred, average='macro'))\n",
    "    \n",
    "    #plot_history(history_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64de9",
   "metadata": {},
   "source": [
    "Below are  functions to save the model and epoch history, and parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729d4323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(network_name):\n",
    "    \n",
    "    global model\n",
    "    \n",
    "    model_json_str = model.to_json()\n",
    "    \n",
    "    json_name = os.path.join(network_name,'model_20.json')\n",
    "    h5_name = os.path.join(network_name,'weights.h5')\n",
    "    open(json_name, 'w').write(model_json_str)\n",
    "    model.save_weights(h5_name)\n",
    "\n",
    "def save_history(network_name):\n",
    "    \n",
    "    global history_x, nb_epoch\n",
    "    \n",
    "    epochs = range(1, nb_epoch+1)\n",
    "    data_history = pd.DataFrame(history_x.history, index = epochs)\n",
    "\n",
    "    print(data_history)\n",
    "    data_history.to_csv(os.path.join(network_name, \"_history\" + \".csv\"))\n",
    "    \n",
    "    \n",
    "def save_parameters(network_name):\n",
    "    \n",
    "    global F1_score, Accuracy, Precision, Recall, confusion_m\n",
    "    \n",
    "    #Final parameters\n",
    "    F1_score = f1_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    Accuracy = accuracy_score(np.argmax(Y_test,1), y_pred)\n",
    "    Precision = precision_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    Recall = recall_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    \n",
    "    parameter_dict = {network_name:[F1_score, Accuracy, Precision, Recall]}\n",
    "    parameter_dataframe = pd.DataFrame(parameter_dict,index=[\"F1_score\", \"Accuracy\", \"Precision\", \"Recall\"])\n",
    "    \n",
    "    print(parameter_dataframe)\n",
    "    parameter_dataframe.to_csv(os.path.join(network_name, \"final_parameter\" + \".csv\"))\n",
    "    \n",
    "    #Confusion matrix\n",
    "    confusion_m = confusion_matrix(np.argmax(Y_test,1), y_pred)\n",
    "    confusion_dataframe = pd.DataFrame(confusion_m, index=[\"Answer:0\", \"Answer:1\"])\n",
    "    confusion_dataframe.columns = [\"Prediction:0\", \"Prediction:1\"]\n",
    "    print(confusion_dataframe)\n",
    "    confusion_dataframe.to_csv(os.path.join(network_name, \"confution_matrix\" + \".csv\"))\n",
    "    \n",
    "    #AUC of ROC curve\n",
    "    print('hello')\n",
    "\n",
    "    prob = model.predict(X_test)[:,1]\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(np.argmax(Y_test,1), prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"AUC: {0}\".format(roc_auc))\n",
    "    roc_data = [fpr, tpr]\n",
    "    roc_dataframe = pd.DataFrame(roc_data, index=[\"False positive rate\", \"True positive rate\"])\n",
    "    print(roc_dataframe)\n",
    "    roc_dataframe.to_csv(os.path.join(network_name, \"ROC_AUC\" + \".csv\"))\n",
    "    #print('hello')\n",
    "    #plt.plot(fpr, tpr, color='red',lw= 2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    #plt.plot([0, 1], [0, 1], color='black', lw= 2, linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    #plt.xlabel('False Positive Rate')\n",
    "    #plt.ylabel('True Positive Rate')\n",
    "    #plt.title('Receiver operating characteristic')\n",
    "    #plt.legend(loc=\"best\")\n",
    "    #print('hello')\n",
    "    #plt.savefig(os.path.join(network_name, \"roc_Auc\" + \".jpg\"))\n",
    "    #plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parmaeters for training\n",
    "\n",
    "n_chan=3            # Number of channels of images\n",
    "\n",
    "scale_v = 128        # Image width/2 (px)\n",
    "scale_h = 128       # Image height/2 (px)\n",
    "\n",
    "test_size = 0.2 #test size\n",
    "nb_epoch = 200 # number of base epochs, not inclusive of additional training sets\n",
    "nb_classes = 2 # number of classes, binary \n",
    "batch_size = 8 # batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f65617",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_npy = np.load('\\\\path\\\\')    #Load numpy file of healthy condition and senescent tagged condition\n",
    "stress_npy = np.load('\\\\path\\\\') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_CNN(control_npy, stress_npy) # train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e478961",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = '\\\\directory to save\\\\'    #Directory path and name to save results\n",
    "save_model(network_name)\n",
    "save_history(network_name)\n",
    "save_parameters(network_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ae73c5",
   "metadata": {},
   "source": [
    "# Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95982b7",
   "metadata": {},
   "source": [
    "Here we will apply the trained model to determine senescent scores of single cells (data instances created from data instance creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dd8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(x0):\n",
    "    return ((x0/255.)-0.5)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5197a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setting(condition):\n",
    "    \n",
    "    global X_test, Y_test, y_pred\n",
    "    \n",
    "    test_X = condition\n",
    "    test_y = np.concatenate((np.tile(np.array([[0]]),(condition.shape[0],1)),\n",
    "                            ),axis=0)\n",
    "    X_test = preprocess_input(np.copy(test_X))\n",
    "    y_test = np.copy(test_y)\n",
    "\n",
    "    X_test = X_test.reshape(X_test.shape[0], scale_v*2, scale_h*2, n_chan)\n",
    "    Y_test = np_utils.to_categorical(y_test, 2)\n",
    "    \n",
    "    Y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "    return X_test, Y_test, Y_pred, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4e520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_parameters(network_name, condition):\n",
    "   \n",
    "    global F1_score, Accuracy, Precision, Recall, confusion_m\n",
    "    \n",
    "    #Final parameters\n",
    "    data_setting(condition)\n",
    "    F1_score = f1_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    Accuracy = accuracy_score(np.argmax(Y_test,1), y_pred)\n",
    "    Precision = precision_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    Recall = recall_score(np.argmax(Y_test,1), y_pred, average='binary')\n",
    "    \n",
    "    parameter_dict = {network_name:[F1_score, Accuracy, Precision, Recall]}\n",
    "    parameter_dataframe = pd.DataFrame(parameter_dict,index=[\"F1_score\", \"Accuracy\", \"Precision\", \"Recall\"])\n",
    "    \n",
    "    print(parameter_dataframe)\n",
    "    parameter_dataframe.to_csv(os.path.join(network_name, \"final_parameter\" + \".csv\"))\n",
    "    \n",
    "    #Confusion matrix\n",
    "    confusion_m = confusion_matrix(np.argmax(Y_test,1), y_pred)\n",
    "    confusion_dataframe = pd.DataFrame(confusion_m, index=[\"Answer:0\", \"Answer:1\",])\n",
    "    confusion_dataframe.columns = [\"Prediction:0\", \"Prediction:1\"]\n",
    "    print(confusion_dataframe)\n",
    "    confusion_dataframe.to_csv(os.path.join(network_name, \"confution_matrix\" + \".csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d273103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prameters for validation\n",
    "\n",
    "n_chan=3            # Number of channels of images\n",
    "\n",
    "scale_v = 128        # Image width/2 (px)\n",
    "scale_h = 128       # Image height/2 (px)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f552f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained CNN data\n",
    "json_file_path = '\\\\path\\\\'\n",
    "h5_file_path = '\\\\path\\\\'\n",
    "model = model_from_json(open(json_file_path).read())\n",
    "model.load_weights(h5_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for prediction\n",
    "data_predict = np.load( '\\\\path\\\\')   #Load numpy file of condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name =  '\\\\path\\\\' #Directory path and name to save results\n",
    "save_parameters(network_name,data_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9345190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test, Y_pred, y_pred=data_setting(data_predict) # predict on the dataset\n",
    "np.save('\\\\path\\\\',Y_pred) # save the scores (note y_pred is the rounded version of this score)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
